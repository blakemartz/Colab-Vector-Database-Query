{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLvPqfXASbcenob4Qi8sAU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blakemartz/Colab-Vector-Database-Query/blob/main/Colab_Vector_Database_Query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data**\n",
        "\n",
        "Github: https://github.com/blakemartz/Colab-Vector-Database-Query"
      ],
      "metadata": {
        "id": "JWtoFPSCfXRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting Started**\n",
        "\n",
        "You can use the open source Llama-2-7b-chat model in both Hugging Face transformers and LangChain. However, you have to first request access to Llama 2 models via Meta website and also accept to share your account details with Meta on Hugging Face website. It typically takes a few minutes or hours to get the access.\n",
        "\n",
        "ðŸš¨ Note that your Hugging Face account email MUST match the email you provided on the Meta website, or your request will not be approved.\n",
        "\n",
        "\n",
        "In your Google Colab notebook, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4. You will need ~8GB of GPU RAM for inference and running on CPU is practically impossible."
      ],
      "metadata": {
        "id": "0LSJSW3igonx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hugging Face token**\n",
        "\n",
        "You need to generate an access token to allow downloading the model from Hugging Face in your code. For that, go to your Hugging Face Profile > Settings > Access Token > New Token > Generate a Token. Just copy the token and add it in the below code\n",
        "\n",
        "**Path to your directory of PDFs**\n",
        "\n",
        "Upload the PDFs you wish to query into a folder on your Google Drive. Update the PDF_directory variable below to the path of that folder."
      ],
      "metadata": {
        "id": "tpU-gwKX_U7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "\n",
        "#Enter your hugging face auth token\n",
        "hf_auth = 'your_token_here'\n",
        "\n",
        "#Enter the path to your directory of PDFs\n",
        "pdf_directory = \"/content/drive/MyDrive/your_directory_path\""
      ],
      "metadata": {
        "id": "vPDpjOPgcbCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the notebook**\n",
        "\n",
        "Go to Runtime > Run all\n",
        "\n",
        "It will take several minutes to download dependencies, process your files, and extract the embeddings to the vector database.\n",
        "\n",
        "**Question Answer**\n",
        "\n",
        "Once all cells have completed, you can scroll to the bottom of the notebook and enter your question in the 'question' variable in the final cell. Run the cell to query the documents.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SkVdn7INHr5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive**"
      ],
      "metadata": {
        "id": "m-HjU3_9G1_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xjwh8OSNBl-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing the Libraries**"
      ],
      "metadata": {
        "id": "uUYJ9_HMh2zf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2uiRWEdWfYc"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers pypdf streamlit streamlit_chat pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing the Hugging Face Pipeline**\n",
        "\n",
        "You have to initialize a text-generation pipeline with Hugging Face transformers. The pipeline requires the following three things that you must initialize:\n",
        "\n",
        "*    A LLM, in this case it will be meta-llama/Llama-2-7b-chat-hf.\n",
        "The respective tokenizer for the model.\n",
        "*    A stopping criteria object.\n",
        "*    You have to initialize the model and move it to CUDA-enabled GPU. Using Colab, this can take 5â€“10 minutes to download and initialize the model.\n"
      ],
      "metadata": {
        "id": "QGWD_NrLYq_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "\n",
        "# the model variable has already set\n",
        "# model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "# model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, you need an access token\n",
        "# hf_auth = 'enter_your_Hugging_Face_access_token'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# enable evaluation mode to allow model inference\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "id": "v1qO3hOPXclH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 7B models were trained using the Llama 2 7B tokenizer, which can be initialized with this code:\n",
        "\n"
      ],
      "metadata": {
        "id": "yN7ndtqQZYPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "9A-FfPmcYhvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to define the stopping criteria of the model. The stopping criteria allows us to specify when the model should stop generating text. If we donâ€™t provide a stopping criteria the model just goes on a bit tangent after answering the initial question."
      ],
      "metadata": {
        "id": "0ruB9QkBZb7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "ydV74l40ZgLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You have to convert these stop token ids into LongTensor objects."
      ],
      "metadata": {
        "id": "jukma_goZw9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "id": "fH5W_Hz0Z2dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do a quick spot check that no <unk> token IDs (0) appear in the stop_token_ids â€” there are none so we can move on to building the stopping criteria object that will check whether the stopping criteria has been satisfied â€” meaning whether any of these token ID combinations have been generated."
      ],
      "metadata": {
        "id": "m245KIfwZ_0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "5c7HFCz9aBlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are ready to initialize the Hugging Face pipeline. There are a few additional parameters that we must define here. Comments are included in the code for further explanation."
      ],
      "metadata": {
        "id": "0sc9EWB5aMKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "hUW-p_4SaM7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this code to confirm that everything is working fine."
      ],
      "metadata": {
        "id": "wx0hwUbJabk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# res = generate_text(\"Explain me the difference between a solar eclipse and a lunar eclipse.\")\n",
        "# print(res[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "6OxNVMMLachl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing HF Pipeline in LangChain**\n",
        "\n",
        "Now, you have to implement the Hugging Face pipeline in LangChain. You will still get the same output as nothing different is being done here. However, this code will allow you to use LangChainâ€™s advanced agent tooling, chains, etc, with Llama 2."
      ],
      "metadata": {
        "id": "MQT2RiMDa9sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "# checking again that everything is working fine\n",
        "# llm(prompt=\"Explain me the difference between a solar eclipse and a lunar eclipse.\")"
      ],
      "metadata": {
        "id": "wQ3-x3NUa_7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ingesting Data using Document Loader**\n",
        "\n",
        "For web based data have to ingest data using WebBaseLoader document loader which collects data by scraping webpages. In this example, you will be collecting data from Databricks documentation website. This is commented out for our PDF directory implementation."
      ],
      "metadata": {
        "id": "Q7-L3eJfbefE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# web_links = [\"https://www.databricks.com/\",\"https://help.databricks.com\",\"https://databricks.com/try-databricks\",\"https://help.databricks.com/s/\",\"https://docs.databricks.com\",\"https://kb.databricks.com/\",\"http://docs.databricks.com/getting-started/index.html\",\"http://docs.databricks.com/introduction/index.html\",\"http://docs.databricks.com/getting-started/tutorials/index.html\",\"http://docs.databricks.com/release-notes/index.html\",\"http://docs.databricks.com/ingestion/index.html\",\"http://docs.databricks.com/exploratory-data-analysis/index.html\",\"http://docs.databricks.com/data-preparation/index.html\",\"http://docs.databricks.com/data-sharing/index.html\",\"http://docs.databricks.com/marketplace/index.html\",\"http://docs.databricks.com/workspace-index.html\",\"http://docs.databricks.com/machine-learning/index.html\",\"http://docs.databricks.com/sql/index.html\",\"http://docs.databricks.com/delta/index.html\",\"http://docs.databricks.com/dev-tools/index.html\",\"http://docs.databricks.com/integrations/index.html\",\"http://docs.databricks.com/administration-guide/index.html\",\"http://docs.databricks.com/security/index.html\",\"http://docs.databricks.com/data-governance/index.html\",\"http://docs.databricks.com/lakehouse-architecture/index.html\",\"http://docs.databricks.com/reference/api.html\",\"http://docs.databricks.com/resources/index.html\",\"http://docs.databricks.com/whats-coming.html\",\"http://docs.databricks.com/archive/index.html\",\"http://docs.databricks.com/lakehouse/index.html\",\"http://docs.databricks.com/getting-started/quick-start.html\",\"http://docs.databricks.com/getting-started/etl-quick-start.html\",\"http://docs.databricks.com/getting-started/lakehouse-e2e.html\",\"http://docs.databricks.com/getting-started/free-training.html\",\"http://docs.databricks.com/sql/language-manual/index.html\",\"http://docs.databricks.com/error-messages/index.html\",\"http://www.apache.org/\",\"https://databricks.com/privacy-policy\",\"https://databricks.com/terms-of-use\"]\n",
        "\n",
        "# loader = WebBaseLoader(web_links)\n",
        "# documents = loader.load()"
      ],
      "metadata": {
        "id": "E-vlty0Dblre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ingesting directory of PDFs using PyPDFDirectoryLoader**\n",
        "\n",
        "How to Use\n",
        "\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "Description\n",
        "\n",
        "Load a directory with `PDF` files using `pypdf` and chunks at character level.\n",
        "\n",
        "Loader also stores page numbers in metadata.\n",
        "\n",
        "**Docs**\n",
        "https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.pdf.PyPDFDirectoryLoader.html"
      ],
      "metadata": {
        "id": "IFulDxMT1NHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "# variable already set in first cell\n",
        "# pdf_directory = \"/content/drive/MyDrive/REPLACE_WITH_YOUR_DIRECTORY\"\n",
        "\n",
        "loader = PyPDFDirectoryLoader(pdf_directory)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "wiroyk8fpGNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting in Chunks using Text Splitters**\n",
        "\n",
        "You have to make sure to split the text into small pieces. You will need to initialize RecursiveCharacterTextSplitter and call it by passing the documents."
      ],
      "metadata": {
        "id": "oTMfPcmNbvgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "all_splits = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "QZeAHNDLbsUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Embeddings and Storing in Vector Store**\n",
        "\n",
        "You have to create embeddings for each small chunk of text and store them in the vector store (i.e. FAISS). You will be using all-mpnet-base-v2 Sentence Transformer to convert all pieces of text in vectors while storing them in the vector store."
      ],
      "metadata": {
        "id": "ljYrA7hDb6ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)\n",
        "\n",
        "# storing embeddings in the vector store\n",
        "vectorstore = FAISS.from_documents(all_splits, embeddings)"
      ],
      "metadata": {
        "id": "L4z4CTNnb9pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initializing Chain**\n",
        "\n",
        "You have to initialize ConversationalRetrievalChain. This chain allows you to have a chatbot with memory while relying on a vector store to find relevant information from your document.\n",
        "\n",
        "Additionally, you can return the source documents used to answer the question by specifying an optional parameter i.e. return_source_documents=True when constructing the chain.\n",
        "\n",
        "*Hint: Re-run this cell to clear your chat history.*"
      ],
      "metadata": {
        "id": "MnEZQN_1clHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n",
        "chat_history = []\n",
        "\n",
        "def get_answer(query):\n",
        "    global chat_history\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result[\"answer\"]))\n",
        "    return result['answer']\n"
      ],
      "metadata": {
        "id": "oQ8ks4LvSAVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**QA your PDFs!**"
      ],
      "metadata": {
        "id": "_PXsLPz2dG39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_len = len(chat_history)\n",
        "for i in range(chat_len):\n",
        "  print(chat_history[i][0])\n",
        "  print(chat_history[i][1])\n",
        "  print(\"\\n\")\n",
        "\n",
        "# Enter your question in the 'question' variable. After each answer enter a enw question here and run the cell again. Chat history will be preserved\n",
        "question = \"Enter your question here and run the cell to query your PDFs.\"\n",
        "\n",
        "print(question)\n",
        "get_answer(question)"
      ],
      "metadata": {
        "id": "rmFuc_pIS1iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "You can also see the source of the information used to generate the answer."
      ],
      "metadata": {
        "id": "-_T9E1D_e4Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['source_documents'])"
      ],
      "metadata": {
        "id": "4GTtyfWTe9Np"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}